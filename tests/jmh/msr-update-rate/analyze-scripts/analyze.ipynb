{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics\n",
    "from sys import argv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get durations in between consecutive timestamps rather than the \n",
    "def diff_list(l):\n",
    "    dl = []\n",
    "    prev = l[0]\n",
    "    for item in l[1:]:\n",
    "        dl.append(item-prev)\n",
    "        prev = item\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulative_sum(l):\n",
    "    sl = []\n",
    "    sum_ = 0\n",
    "    for duration in l:\n",
    "        sum_ += duration\n",
    "        sl.append(sum_)\n",
    "    return sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_for_outliers(unfiltered, **kwargs):\n",
    "    allowed_kws = {\"lt\", \"gt\", \"ge\", \"le\", \"eq\"}\n",
    "    kws_provided = set(kwargs.keys())\n",
    "    diff = kws_provided - allowed_kws\n",
    "    if diff:\n",
    "        raise ValueError(f\"Invalid keyword(s) provided: {diff}\")\n",
    "    filtered = []\n",
    "    outliers = []\n",
    "    for val in unfiltered:\n",
    "        append_val = True\n",
    "        if \"lt\" in kws_provided:\n",
    "            append_val &= (val < kwargs[\"lt\"])\n",
    "        if \"gt\" in kws_provided:\n",
    "            append_val &= (val > kwargs[\"gt\"])\n",
    "        if \"ge\" in kws_provided:\n",
    "            append_val &= (val >= kwargs[\"ge\"])\n",
    "        if \"le\" in kws_provided:\n",
    "            append_val &= (val <= kwargs[\"le\"])\n",
    "        if \"eq\" in kws_provided:\n",
    "            append_val &= (val == kwargs[\"eq\"])\n",
    "        if append_val: filtered.append(val)\n",
    "        else: outliers.append(val)\n",
    "    return filtered, outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read the csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['dram', 'pkg', 'dram.1', 'pkg.1']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('jolteon-samples.csv')\n",
    "df = df.loc[:, (df != 0).any(axis=0)]\n",
    "print('read the csv')\n",
    "header = list(df.head())\n",
    "del header[-1] # remove timestamp\n",
    "timestamps = df['timestamp']\n",
    "header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collected result dataframe for dram. len(outliers)=100,len(result)=28266\n",
      "collected result dataframe for pkg. len(outliers)=10,len(result)=49688\n",
      "collected result dataframe for dram.1. len(outliers)=10,len(result)=49229\n",
      "collected result dataframe for pkg.1. len(outliers)=10,len(result)=49689\n",
      "all results stored in 'result_dfs', currently a dictionary\n"
     ]
    }
   ],
   "source": [
    "result_dfs = dict()\n",
    "\n",
    "for powerDomain in header:\n",
    "    energies = df[powerDomain]\n",
    "    lastDifferent = energies[0]\n",
    "    change_ts = [ timestamps[0] ] # timestamps where there was an energy update\n",
    "    ener_diffs = [] # differences between consecutive non-equal readings\n",
    "    n_samples_bw = [] # number of samples between consecutive non-equal readings\n",
    "    non_zero_reading_nums = [1]\n",
    "    bw = 0\n",
    "    for i in range(len(energies)):\n",
    "        if energies[i] != lastDifferent:\n",
    "            non_zero_reading_nums.append(i+1)\n",
    "            ener_diffs.append(energies[i] - lastDifferent)\n",
    "            change_ts.append(timestamps[i])\n",
    "            n_samples_bw.append(bw)\n",
    "            lastDifferent = energies[i]\n",
    "            bw = 0\n",
    "        else:\n",
    "            bw += 1\n",
    "    # Get the difference between consecutive timestamps\n",
    "    ts_diffs = diff_list(change_ts)\n",
    "    # Get the cumulative sum of the unfiltered duractions\n",
    "    cum_sums_unfiltered = cumulative_sum(ts_diffs) # get durations instead of raw timestamps\n",
    "    # Get the filtered durations <=5000 and outliers >5000\n",
    "    filtered, outliers = filter_for_outliers(ts_diffs, le=5000)\n",
    "    # Get the cumulative sum of the filtered timestamps\n",
    "    cum_sums_filtered = cumulative_sum(filtered)\n",
    "    \n",
    "    result = filtered\n",
    "    \n",
    "    result_dfs[powerDomain] = {\n",
    "                               \"change_timestamps\": np.array(change_ts),\n",
    "                               \"reading_change_num\": np.array(non_zero_reading_nums),\n",
    "                               \"energy_differences\": np.array(ener_diffs),\n",
    "                               \"readings_bw_change\": np.array(n_samples_bw),\n",
    "                               \"filtered\": np.array(filtered, dtype=np.double),\n",
    "                               \"outliers\": np.array(outliers),\n",
    "                               \"cumulative_sum_unfiltered\": np.array(cum_sums_unfiltered),\n",
    "                               \"cumulative_sum_filtered\": np.array(cum_sums_filtered)\n",
    "                              }  \n",
    "    \n",
    "    print(f'collected result dataframe for {powerDomain}. len(outliers)={len(outliers)},len(result)={len(result)}')\n",
    "    \n",
    "print('all results stored in \\'result_dfs\\', currently a dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error plotting results_dfs[dram]\n",
      "error plotting results_dfs[pkg]\n",
      "error plotting results_dfs[dram.1]\n",
      "error plotting results_dfs[pkg.1]\n"
     ]
    }
   ],
   "source": [
    "for key in result_dfs.keys():\n",
    "    try:\n",
    "        plot = results_dfs[key].plot()\n",
    "        plot.set_title(f'time to update the MSR for {key.upper()}')\n",
    "        plot.get_figure().savefig(f'{key}_msr-update-time.png')\n",
    "        # plot.get_figure().clf()\n",
    "        with open(key+'_summary-statistics.tex','w') as fh: fh.write(results_dfs[key].describe().to_latex())\n",
    "    except:\n",
    "        print(f\"error plotting results_dfs[{key}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_dpi = 1000\n",
    "fname = \"trial1_\"\n",
    "for domain in result_dfs.keys():\n",
    "    change_ts = result_dfs[domain][\"change_timestamps\"]\n",
    "    non_zero_readings = result_dfs[domain][\"reading_change_num\"]\n",
    "    ener_diffs = result_dfs[domain][\"energy_differences\"]\n",
    "    n_samples_bw = result_dfs[domain][\"readings_bw_change\"]\n",
    "    filtered = result_dfs[domain][\"filtered\"]\n",
    "    outliers = result_dfs[domain][\"outliers\"]\n",
    "    cum_sums_unfiltered = result_dfs[domain][\"cumulative_sum_unfiltered\"]\n",
    "    cum_sums_filtered = result_dfs[domain][\"cumulative_sum_filtered\"]\n",
    "    domain = domain.split('.')\n",
    "    if len(domain) == 2:\n",
    "        domain = f\"{domain[0]} Socket 1\"\n",
    "    else:\n",
    "        domain = f\"{domain[0]} Socket 0\"\n",
    "    domain_fname = f\"rutvik_results/{'_'.join(domain.split())}\"\n",
    "    stats_d = {}\n",
    "    \n",
    "    ### Calculating correlation coeffiecients for all pairs of reading_value, readings_bw_change, num_unique_readings\n",
    "    corr_coeffs = stats_d[\"CorrCoeffs\"] = {}\n",
    "    \n",
    "    cc_readings_samples_bw = np.corrcoef(ener_diffs, n_samples_bw)[0][1]\n",
    "    corr_coeffs[\"ener_diff-vs-num_samples_bw_nonequal\"] = cc_readings_samples_bw\n",
    "    \n",
    "    cc_n_samples_bw = np.corrcoef(range(len(ener_diffs)), n_samples_bw)[0][1]\n",
    "    corr_coeffs['unique_reading_num-vs-num_samples_bw_nonequal'] = cc_n_samples_bw\n",
    "    \n",
    "    cc_n_readings = np.corrcoef(range(len(ener_diffs)), ener_diffs)[0][1]\n",
    "    corr_coeffs['unique_reading_num-vs-ener_diff'] = cc_n_readings\n",
    "    \n",
    "    ### Stats like mean and sd\n",
    "    stats_d['num_readings'] = len(cum_sums_unfiltered) + 1\n",
    "    \n",
    "    stats_d[\"avg_readings_bw_2_nonequal\"] = np.mean(n_samples_bw)\n",
    "    stats_d[\"s-d_readings_bw_2_nonequal\"] = np.std(n_samples_bw)\n",
    "    \n",
    "    stats_d['avg_energy_reading'] = np.mean(ener_diffs)\n",
    "    stats_d['s-d_energy_reading'] = np.std(ener_diffs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###Plots\n",
    "    \n",
    "    # Not sure - autocorelation for the filtered values\n",
    "    # plt.acorr(filtered, usevlines=True, normed=True, maxlags=10, lw=2)\n",
    "    # plt.xlabel(\"idk\")\n",
    "    # plt.ylabel(\"idk\")\n",
    "    # plt.title(f\"Autocorrelation filtered times- {domain}\")\n",
    "    # plt.savefig()\n",
    "    # plt.close(plt.gcf())\n",
    "    \n",
    "    # scatter plot time for value x vs time for value x+1  - filtered\n",
    "    plt.scatter(filtered[:-1], filtered[1:], marker='.', s=3)\n",
    "    plt.xlabel(\"Time for reading i\")\n",
    "    plt.ylabel(\"Time for reading i+1\")\n",
    "    plt.title(f\"Consecutive reading times - {domain}\")\n",
    "    plt.savefig(f\"{domain_fname}-i_n-v-i_n1\",  bbox_inches = 'tight', dpi=my_dpi)\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    \n",
    "    # filtered cumulative sum line plot\n",
    "    plt.plot(cum_sums_filtered)\n",
    "    plt.xlabel(\"Reading number\")\n",
    "    plt.ylabel(\"Cumulative time\")\n",
    "    plt.title(f\"Cumulative Summed Line - {domain}\")\n",
    "    plt.savefig(f\"{domain_fname}-cumulative-summed\",  bbox_inches = 'tight', dpi=my_dpi)\n",
    "    plt.cla()\n",
    "    plt.clf()\n",
    "    \n",
    "    json.dump(stats_d, open(f\"{domain_fname}_stats.json\", 'w'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CorrCoeffs': {'ener_diff-vs-num_samples_bw_nonequal': -0.03648222165532877,\n",
       "  'unique_reading_num-vs-num_samples_bw_nonequal': 0.3133872125639214,\n",
       "  'unique_reading_num-vs-ener_diff': 0.0022845401948618956},\n",
       " 'num_readings': 49700,\n",
       " 'avg_readings_bw_2_nonequal': 46.73502484959456,\n",
       " 's-d_readings_bw_2_nonequal': 4.585179884126291,\n",
       " 'avg_energy_reading': 0.029396382220970442,\n",
       " 's-d_energy_reading': 0.5899954497038302}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    stats_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
